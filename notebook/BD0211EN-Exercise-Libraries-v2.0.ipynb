{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h1 align = \"center\"> Spark Fundamentals I - Introduction to Spark</h1>\n",
    "<h2 align = \"center\"> Scala - Working with Scala Libraries</h2>\n",
    "<br align = \"left\">\n",
    "\n",
    "**Related free online courses:**\n",
    "\n",
    "Related courses can be found in the following learning paths:\n",
    "\n",
    "- [Spark Fundamentals path](http://cocl.us/Spark_Fundamentals_Path)\n",
    "- [Big Data Fundamentals path](http://cocl.us/Big_Data_Fundamentals_Path)\n",
    "\n",
    "<img src=\"http://spark.apache.org/images/spark-logo.png\" height=100>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Note: Using python cause Scala sucks\n",
    "\n",
    "\n",
    "## Creating a Spark application using Spark SQL\n",
    "\n",
    "Spark SQL provides the ability to write relational queries to be run on Spark. There is the abstraction SchemaRDD which is to create an RDD in which you can run SQL, HiveQL, and Scala. In this lab section, you will use SQL to find out the average weather and precipitation for a given time period in New York. The purpose is to demonstrate how to use the Spark SQL libraries on Spark.\n",
    "\n",
    "### Please note that in Spark 1.3 DataFrames have replaced schemaRDDs however, it is still possible to switch between the two for supporting legacy systems. DataFrames is the recommended method going forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Disabled as the file needed is commited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# download module to run shell commands within this notebook\n",
    "# import sys.process._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# download data from IBM Servier\n",
    "# this may take ~30 seconds depending on your internet speed\n",
    "# \"wget --quiet https://ibm.box.com/shared/static/j8skrriqeqw66f51iyz911zyqai64j2g.zip\" !\n",
    "\n",
    "# println(\"Data Downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's unzip the data that we just downloaded into a directory dedicated for this course. Let's choose the directory **/resources/jupyter/labs/BD0211EN/**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# unzip the folder's content into \"resources\" directory\n",
    "# \"unzip -q -o -d /resources/jupyterlab/labs/BD0211EN/ j8skrriqeqw66f51iyz911zyqai64j2g.zip\" !\n",
    "\n",
    "# println(\"Data Extracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The data is in a folder called **LabData**. Let's list all the files in the data that we just downloaded and extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BD0211EN-Exercise-Dataframes-py.ipynb\r\n",
      "BD0211EN-Exercise-Getting-Started-py-v2.0.ipynb\r\n",
      "BD0211EN-Exercise-RDD-py-v2.0.ipynb\r\n",
      "BD0211EN-Exercise-Scala-Libraries-sc-v2.0.ipynb\r\n",
      "README.md\r\n",
      "data\r\n",
      "j8skrriqeqw66f51iyz911zyqai64j2g.zip\r\n",
      "mtcars.csv\r\n",
      "notebook.log\r\n",
      "pom.xml\r\n"
     ]
    }
   ],
   "source": [
    "# list the extracted files\n",
    "!ls -1 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /Users/wei/.pyenv/versions/3.7.5/envs/jupyter/lib/python3.7/site-packages (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's take a look at the nycweather data. So run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\"2013-01-01\"', '1', '0'],\n",
       " ['\"2013-01-02\"', '-2', '0'],\n",
       " ['\"2013-01-03\"', '-2', '0'],\n",
       " ['\"2013-01-04\"', '1', '0'],\n",
       " ['\"2013-01-05\"', '3', '0'],\n",
       " ['\"2013-01-06\"', '4', '0'],\n",
       " ['\"2013-01-07\"', '5', '0'],\n",
       " ['\"2013-01-08\"', '6', '0'],\n",
       " ['\"2013-01-09\"', '7', '0'],\n",
       " ['\"2013-01-10\"', '7', '0']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatherRdd = sc.textFile(\"nycweather.csv\").map(lambda l: l.split(\",\"))\n",
    "weatherRdd.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "There are three columns in the dataset, the date, the mean temperature in Celsius, and the precipitation for the day. Since we already know the schema, we will infer the schema using reflection.\n",
    "\n",
    "You will first need to define the SparkSQL context. Do so by creating it from an existing SparkContext. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sqlContext = pyspark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert RDD to `Row` objects for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"2013-01-01\" 1 0',\n",
       " '\"2013-01-02\" -2 0',\n",
       " '\"2013-01-03\" -2 0',\n",
       " '\"2013-01-04\" 1 0',\n",
       " '\"2013-01-05\" 3 0',\n",
       " '\"2013-01-06\" 4 0',\n",
       " '\"2013-01-07\" 5 0',\n",
       " '\"2013-01-08\" 6 0',\n",
       " '\"2013-01-09\" 7 0',\n",
       " '\"2013-01-10\" 7 0']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatherRdd.map(lambda r: f\"{r[0]} {r[1]} {r[2]}\").take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date='\"2013-01-01\"', precipitation='0', temp='1'),\n",
       " Row(date='\"2013-01-02\"', precipitation='0', temp='-2'),\n",
       " Row(date='\"2013-01-03\"', precipitation='0', temp='-2'),\n",
       " Row(date='\"2013-01-04\"', precipitation='0', temp='1'),\n",
       " Row(date='\"2013-01-05\"', precipitation='0', temp='3'),\n",
       " Row(date='\"2013-01-06\"', precipitation='0', temp='4'),\n",
       " Row(date='\"2013-01-07\"', precipitation='0', temp='5'),\n",
       " Row(date='\"2013-01-08\"', precipitation='0', temp='6'),\n",
       " Row(date='\"2013-01-09\"', precipitation='0', temp='7'),\n",
       " Row(date='\"2013-01-10\"', precipitation='0', temp='7')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatherRows = weatherRdd.map(lambda r: pyspark.sql.Row(date=r[0], temp=r[1], precipitation=r[2]))\n",
    "weatherRows.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn RDD into data frame and register as table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "weatherDf = sqlContext.createDataFrame(weatherRows)\n",
    "sqlContext.registerDataFrameAsTable(weatherDf, \"weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "At this point, you are ready to create and run some queries on the DatatFrame. You want to get a list of the hottest dates with some precipitation. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date='\"2013-01-30\"', precipitation='1.02', temp='9'),\n",
       " Row(date='\"2013-03-31\"', precipitation='2.03', temp='9'),\n",
       " Row(date='\"2013-11-27\"', precipitation='50.29', temp='9'),\n",
       " Row(date='\"2013-01-14\"', precipitation='2.29', temp='8'),\n",
       " Row(date='\"2013-01-31\"', precipitation='22.86', temp='8'),\n",
       " Row(date='\"2013-01-12\"', precipitation='0.51', temp='7'),\n",
       " Row(date='\"2013-04-12\"', precipitation='16', temp='7'),\n",
       " Row(date='\"2013-01-11\"', precipitation='13.97', temp='6'),\n",
       " Row(date='\"2013-01-29\"', precipitation='1.52', temp='6'),\n",
       " Row(date='\"2013-02-27\"', precipitation='39.62', temp='6'),\n",
       " Row(date='\"2013-11-12\"', precipitation='0.76', temp='6'),\n",
       " Row(date='\"2013-02-19\"', precipitation='3.81', temp='5'),\n",
       " Row(date='\"2013-02-24\"', precipitation='0.25', temp='5'),\n",
       " Row(date='\"2013-02-11\"', precipitation='12.45', temp='4'),\n",
       " Row(date='\"2013-02-13\"', precipitation='0.76', temp='4'),\n",
       " Row(date='\"2013-02-23\"', precipitation='6.6', temp='4'),\n",
       " Row(date='\"2013-02-26\"', precipitation='3.56', temp='4'),\n",
       " Row(date='\"2013-11-26\"', precipitation='12.95', temp='4'),\n",
       " Row(date='\"2013-01-15\"', precipitation='3.05', temp='3'),\n",
       " Row(date='\"2013-03-08\"', precipitation='14.22', temp='3'),\n",
       " Row(date='\"2013-03-16\"', precipitation='3.05', temp='3'),\n",
       " Row(date='\"2013-03-19\"', precipitation='9.14', temp='3'),\n",
       " Row(date='\"2013-03-25\"', precipitation='4.32', temp='3'),\n",
       " Row(date='\"2013-12-07\"', precipitation='3.56', temp='3'),\n",
       " Row(date='\"2013-06-26\"', precipitation='1.27', temp='27'),\n",
       " Row(date='\"2013-06-27\"', precipitation='6.1', temp='27'),\n",
       " Row(date='\"2013-07-08\"', precipitation='5.59', temp='27'),\n",
       " Row(date='\"2013-07-09\"', precipitation='5.84', temp='27'),\n",
       " Row(date='\"2013-07-22\"', precipitation='1.52', temp='27'),\n",
       " Row(date='\"2013-07-23\"', precipitation='7.87', temp='27'),\n",
       " Row(date='\"2013-08-09\"', precipitation='1.27', temp='27'),\n",
       " Row(date='\"2013-06-02\"', precipitation='21.59', temp='26'),\n",
       " Row(date='\"2013-07-03\"', precipitation='13.46', temp='26'),\n",
       " Row(date='\"2013-08-27\"', precipitation='0.25', temp='26'),\n",
       " Row(date='\"2013-08-28\"', precipitation='10.92', temp='26'),\n",
       " Row(date='\"2013-09-02\"', precipitation='1.27', temp='26'),\n",
       " Row(date='\"2013-09-10\"', precipitation='0.25', temp='26'),\n",
       " Row(date='\"2013-09-12\"', precipitation='40.64', temp='26'),\n",
       " Row(date='\"2013-06-17\"', precipitation='0.25', temp='25'),\n",
       " Row(date='\"2013-07-02\"', precipitation='2.03', temp='25'),\n",
       " Row(date='\"2013-07-29\"', precipitation='0.25', temp='25'),\n",
       " Row(date='\"2013-07-01\"', precipitation='21.34', temp='24'),\n",
       " Row(date='\"2013-08-08\"', precipitation='11.68', temp='24'),\n",
       " Row(date='\"2013-08-12\"', precipitation='1.27', temp='24'),\n",
       " Row(date='\"2013-08-22\"', precipitation='6.35', temp='24'),\n",
       " Row(date='\"2013-08-26\"', precipitation='1.02', temp='24'),\n",
       " Row(date='\"2013-09-03\"', precipitation='0.76', temp='24'),\n",
       " Row(date='\"2013-06-18\"', precipitation='4.83', temp='23'),\n",
       " Row(date='\"2013-07-12\"', precipitation='6.35', temp='23'),\n",
       " Row(date='\"2013-07-13\"', precipitation='1.52', temp='23'),\n",
       " Row(date='\"2013-07-28\"', precipitation='6.1', temp='23'),\n",
       " Row(date='\"2013-08-03\"', precipitation='1.52', temp='23'),\n",
       " Row(date='\"2013-08-13\"', precipitation='21.59', temp='23'),\n",
       " Row(date='\"2013-05-23\"', precipitation='45.97', temp='22'),\n",
       " Row(date='\"2013-06-03\"', precipitation='22.1', temp='22'),\n",
       " Row(date='\"2013-06-11\"', precipitation='2.29', temp='22'),\n",
       " Row(date='\"2013-08-01\"', precipitation='16.51', temp='22'),\n",
       " Row(date='\"2013-05-10\"', precipitation='0.25', temp='21'),\n",
       " Row(date='\"2013-09-21\"', precipitation='18.29', temp='21'),\n",
       " Row(date='\"2013-06-19\"', precipitation='0.25', temp='20'),\n",
       " Row(date='\"2013-09-13\"', precipitation='1.52', temp='20'),\n",
       " Row(date='\"2013-10-07\"', precipitation='6.35', temp='20'),\n",
       " Row(date='\"2013-01-16\"', precipitation='17.53', temp='2'),\n",
       " Row(date='\"2013-02-16\"', precipitation='0.51', temp='2'),\n",
       " Row(date='\"2013-03-07\"', precipitation='4.83', temp='2'),\n",
       " Row(date='\"2013-12-09\"', precipitation='7.62', temp='2'),\n",
       " Row(date='\"2013-12-15\"', precipitation='18.29', temp='2'),\n",
       " Row(date='\"2013-05-11\"', precipitation='27.69', temp='19'),\n",
       " Row(date='\"2013-06-08\"', precipitation='12.19', temp='19'),\n",
       " Row(date='\"2013-06-10\"', precipitation='35.05', temp='19'),\n",
       " Row(date='\"2013-07-25\"', precipitation='0.25', temp='19'),\n",
       " Row(date='\"2013-10-17\"', precipitation='0.51', temp='19'),\n",
       " Row(date='\"2013-04-10\"', precipitation='12.45', temp='18'),\n",
       " Row(date='\"2013-06-06\"', precipitation='3.3', temp='18'),\n",
       " Row(date='\"2013-09-16\"', precipitation='0.76', temp='18'),\n",
       " Row(date='\"2013-10-11\"', precipitation='0.51', temp='18'),\n",
       " Row(date='\"2013-11-01\"', precipitation='3.3', temp='18'),\n",
       " Row(date='\"2013-04-17\"', precipitation='0.51', temp='17'),\n",
       " Row(date='\"2013-04-19\"', precipitation='1.27', temp='17'),\n",
       " Row(date='\"2013-05-28\"', precipitation='13.21', temp='17'),\n",
       " Row(date='\"2013-06-13\"', precipitation='32', temp='17'),\n",
       " Row(date='\"2013-06-14\"', precipitation='9.65', temp='17'),\n",
       " Row(date='\"2013-09-22\"', precipitation='11.43', temp='17'),\n",
       " Row(date='\"2013-05-09\"', precipitation='12.7', temp='16'),\n",
       " Row(date='\"2013-05-18\"', precipitation='0.25', temp='16'),\n",
       " Row(date='\"2013-06-07\"', precipitation='105.66', temp='16'),\n",
       " Row(date='\"2013-10-31\"', precipitation='1.52', temp='16'),\n",
       " Row(date='\"2013-05-08\"', precipitation='76.71', temp='15'),\n",
       " Row(date='\"2013-05-19\"', precipitation='15.24', temp='14'),\n",
       " Row(date='\"2013-10-19\"', precipitation='0.25', temp='14'),\n",
       " Row(date='\"2013-11-18\"', precipitation='5.59', temp='14'),\n",
       " Row(date='\"2013-12-21\"', precipitation='0.25', temp='14'),\n",
       " Row(date='\"2013-04-18\"', precipitation='0.25', temp='13'),\n",
       " Row(date='\"2013-04-29\"', precipitation='1.02', temp='13'),\n",
       " Row(date='\"2013-05-24\"', precipitation='7.62', temp='13'),\n",
       " Row(date='\"2013-11-17\"', precipitation='0.76', temp='13'),\n",
       " Row(date='\"2013-11-07\"', precipitation='3.3', temp='12'),\n",
       " Row(date='\"2013-11-16\"', precipitation='1.27', temp='12'),\n",
       " Row(date='\"2013-11-22\"', precipitation='1.78', temp='12'),\n",
       " Row(date='\"2013-12-05\"', precipitation='0.25', temp='12'),\n",
       " Row(date='\"2013-04-20\"', precipitation='1.52', temp='11'),\n",
       " Row(date='\"2013-03-12\"', precipitation='20.07', temp='10'),\n",
       " Row(date='\"2013-04-13\"', precipitation='0.25', temp='10'),\n",
       " Row(date='\"2013-05-25\"', precipitation='3.56', temp='10'),\n",
       " Row(date='\"2013-12-06\"', precipitation='18.54', temp='10'),\n",
       " Row(date='\"2013-01-28\"', precipitation='5.59', temp='1'),\n",
       " Row(date='\"2013-12-10\"', precipitation='5.84', temp='1'),\n",
       " Row(date='\"2013-03-18\"', precipitation='15.24', temp='0'),\n",
       " Row(date='\"2013-01-25\"', precipitation='1.78', temp='-7'),\n",
       " Row(date='\"2013-02-02\"', precipitation='0.51', temp='-4'),\n",
       " Row(date='\"2013-02-03\"', precipitation='0.51', temp='-3'),\n",
       " Row(date='\"2013-02-09\"', precipitation='9.65', temp='-3'),\n",
       " Row(date='\"2013-12-14\"', precipitation='18.54', temp='-2'),\n",
       " Row(date='\"2013-12-17\"', precipitation='4.83', temp='-2'),\n",
       " Row(date='\"2013-02-05\"', precipitation='0.51', temp='-1'),\n",
       " Row(date='\"2013-02-08\"', precipitation='29.21', temp='-1'),\n",
       " Row(date='\"2013-12-08\"', precipitation='2.03', temp='-1')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlContext.sql(\"SELECT * FROM weather WHERE precipitation > 0.0 ORDER BY temp DESC\")\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Creating a Spark application using MLlib\n",
    "\n",
    "In this section, Spark will be used to acquire the K-Means clustering for drop-off latitudes and longitudes of taxis for 3 clusters. The sample data contains a subset of taxi trips with hack license, medallion, pickup date/time, drop off date/time, pickup/drop off latitude/longitude, passenger count, trip distance, trip time and other information. As such, this may give a good indication of where to best to hail a cab.\n",
    "\n",
    "Remember, this is only a subset of the file that you used in a previous exercise. If you ran this exercise on the full dataset, it would take a long time as we are only running on a test environment with limited resources.\n",
    "\n",
    "Import the needed packages for K-Means algorithm and Vector packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.clustering.KMeans\n",
    "import org.apache.spark.mllib.linalg.Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Create an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "val taxiFile = sc.textFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/nyctaxisub.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Determine the number of rows in taxiFile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "taxiFile.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Cleanse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "val taxiData=taxiFile.filter(_.contains(\"2013\")).\n",
    "    filter(_.split(\",\")(3)!=\"\" ).    //dropoff_latitude\n",
    "    filter(_.split(\",\")(4)!=\"\")      //dropoff_longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The first filter limits the rows to those that occurred in the year 2013. This will also remove any header in the file. The third and fourth columns contain the drop off latitude and longitude. The transformation will throw exceptions if these values are empty.\n",
    "\n",
    "Do another count to see what was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "taxiData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this case, if we had used the full set of data, it would have filtered out a great many more lines.\n",
    "\n",
    "To fence the area roughly to New York City use this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "val taxiFence=taxiData.\n",
    "    filter(_.split(\",\")(3).toDouble>40.70).\n",
    "    filter(_.split(\",\")(3).toDouble<40.86).\n",
    "    filter(_.split(\",\")(4).toDouble>(-74.02)).\n",
    "    filter(_.split(\",\")(4).toDouble<(-73.93))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Determine how many are left in taxiFence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "taxiFence.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Approximately, 43,354 rows were dropped since these drop-off points are outside of New York City.\n",
    "\n",
    "Create Vectors with the latitudes and longitudes that will be used as input to the K-Means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "val taxi=taxiFence.\n",
    "    map{\n",
    "        line=>Vectors.dense(\n",
    "            line.split(',').slice(3,5).map(_ .toDouble)\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "val iterationCount=10\n",
    "val clusterCount=3\n",
    "\n",
    "val model=KMeans.train(taxi,clusterCount,iterationCount)\n",
    "val clusterCenters=model.clusterCenters.map(_.toArray)\n",
    "\n",
    "clusterCenters.foreach(lines=>println(lines(0),lines(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now we know the map co-ordinates. Not surprisingly, the second point is between the Theater District and Grand Central. The third point is in The Village, NYU, Soho and Little Italy area. The first point is the Upper East Side, presumably where people are more likely to take cabs than subways.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Creating a Spark application using Spark Streaming\n",
    "\n",
    "This section focuses on Spark Streams, an easy to build, scalable, stateful (e.g. sliding windows) stream processing library. Streaming jobs are written the same way Spark batch jobs are coded and support Java, Scala and Python. In this exercise, taxi trip data will be streamed using a socket connection and then analyzed to provide a summary of number of passengers by taxi vendor. This will be implemented in the Spark shell using Scala.\n",
    "\n",
    "There are two relevant files for this section. The first one is the `nyctaxi100.csv` which will serve as the source of the stream. The other file is a python file, `taxistreams.py`, which will feed the csv file through a socket connection to simulate a stream.\n",
    "\n",
    "Run on command line: `python taxistreams.py`\n",
    "Verify data is being sent: `nc 127.0.0.1 7777`\n",
    "\n",
    "Once started, the program will bind and listen to the localhost socket 7777. When a connection is made, it will read ‘nyctaxi100.csv’ and send across the socket. The sleep is set such that one line will be sent every 0.5 seconds, or 2 rows a second. This was intentionally set to a high value to make it easier to view the data during execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Create the StreamingContext by using the existing SparkContext (sc). It will be using a 1 second batch interval, which means the stream is divided to 1 second batches and each batch becomes a RDD. This is intentional to make it easier to read the data during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Create the socket stream that connects to the localhost socket 7777. This matches the port that the Python script is listening on. Each batch from the Stream be a lines RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "taxiLines = ssc.socketTextStream(\"127.0.0.1\", 7777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, put in the business logic to split up the lines on each comma and mapping pass(15), which is the vendor, and pass(7), which is the passenger count. Then this is reduced by key resulting in a summary of number of passengers by vendor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "passCount = taxiLines \\\n",
    "    .map(lambda l: l.split(\",\")) \\\n",
    "    .map(lambda p: (p[15], int(p[7]))) \\\n",
    "    .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Print out to the console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "passCount.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The next two line starts the stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:15:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:15:58\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:15:59\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:01\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:02\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:04\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:06\n",
      "-------------------------------------------\n",
      "('\"VTS\"', 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:07\n",
      "-------------------------------------------\n",
      "('\"VTS\"', 5)\n",
      "('\"CMT\"', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:08\n",
      "-------------------------------------------\n",
      "('\"VTS\"', 1)\n",
      "('\"CMT\"', 5)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:09\n",
      "-------------------------------------------\n",
      "('\"VTS\"', 2)\n",
      "('\"CMT\"', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:10\n",
      "-------------------------------------------\n",
      "('\"VTS\"', 4)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:11\n",
      "-------------------------------------------\n",
      "('\"VTS\"', 1)\n",
      "('\"CMT\"', 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:12\n",
      "-------------------------------------------\n",
      "('\"VTS\"', 4)\n",
      "('\"CMT\"', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:13\n",
      "-------------------------------------------\n",
      "('\"CMT\"', 6)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:14\n",
      "-------------------------------------------\n",
      "('\"CMT\"', 3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:15\n",
      "-------------------------------------------\n",
      "('\"VTS\"', 3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:16\n",
      "-------------------------------------------\n",
      "('\"CMT\"', 4)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:17\n",
      "-------------------------------------------\n",
      "('\"VTS\"', 6)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:18\n",
      "-------------------------------------------\n",
      "('\"CMT\"', 4)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:19\n",
      "-------------------------------------------\n",
      "('\"VTS\"', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:22\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:23\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:26\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:28\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:29\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-29 09:16:31\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-18f3db416f1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "It will take a few cycles for the connection to be recognized, and then the data is sent. In this case, 2 rows per second of taxi trip data is receive in a 1 second batch interval.\n",
    "\n",
    "In the Python terminal, the contents of the file are printed as they are streamed.\n",
    "\n",
    "**Note: TO STOP THE STREAM PLEASE INTERRUPT THE KERNEL IN BOTH THE OTHER PYTHON NOTEBOOK AND THIS NOTEBOOK. THEN RESTART THIS NOTEBOOK'S KERNEL TO CONTINUE ONTO THE GRAPHX APPLICATION**\n",
    "\n",
    "This is just a simple example showing how you can take streaming data into Spark and do some type of processing on it. In the case here, the taxi and the number of passengers was extracted from the data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Creating a Spark application using GraphX\n",
    "\n",
    "Users.txt is a set of users and followers is the relationship between the users. Take a look at the contents of these two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "println(\"Users: \")\n",
    "println(scala.io.Source.fromFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/users.txt\").mkString)\n",
    "\n",
    "println(\"Followers: \")\n",
    "println(scala.io.Source.fromFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/followers.txt\").mkString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Import the GraphX package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.graphx._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Create the users RDD and parse into tuples of user id and attribute list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "val users = (sc.textFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/users.txt\").map(line => line.split(\",\")).map(parts => (parts.head.toLong, parts.tail)))\n",
    "\n",
    "users.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Parse the edge data, which is already in userId -> userId format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "val followerGraph = GraphLoader.edgeListFile(sc, \"/resources/jupyterlab/labs/BD0211EN/LabData/followers.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Attach the user attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "val graph = followerGraph.outerJoinVertices(users) {\n",
    "    case (uid, deg, Some(attrList)) => attrList\n",
    "    case (uid, deg, None) => Array.empty[String]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Restrict the graph to users with usernames and names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "val subgraph = graph.subgraph(vpred = (vid, attr) => attr.size == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Compute the PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "val pagerankGraph = subgraph.pageRank(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Get the attributes of the top pagerank users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "val userInfoWithPageRank = subgraph.outerJoinVertices(pagerankGraph.vertices) {\n",
    "    case (uid, attrList, Some(pr)) => (pr, attrList.toList)\n",
    "    case (uid, attrList, None) => (0.0, attrList.toList)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Print the line out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "println(userInfoWithPageRank.vertices.top(5)(Ordering.by(_._2._1)).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "    <strong>Tip</strong>: Enjoyed using Jupyter notebooks with Spark? Get yourself a free \n",
    "    <a href=\"http://cocl.us/DSX_on_Cloud\">IBM Cloud</a> account where you can use Data Science Experience notebooks\n",
    "    and have <em>two</em> Spark executors for free!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "Having completed this exercise, you should have some familiarity with using the Spark libraries. In particular, you use Spark SQL to effectively query data inside of Spark. You used Spark Streaming to process incoming streams of batch data. You used Spark's MLlib to compute the *k*-means algorithm to find the best place to hail a cab. Finally, you used Spark's GraphX library to perform and parallel graph calculations on a dataset to find the attributes of the top users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This notebook is part of the free course on **Cognitive Class** called *Spark Fundamentals I*. If you accessed this notebook outside the course, you can take this free self-paced course, online by going to: http://cocl.us/Spark_Fundamentals_I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### About the Authors:  \n",
    "Hi! It's Alex Aklson, one of the authors of this notebook. I hope you found this lab educational! There is much more to learn about Spark but you are well on your way. Feel free to connect with me if you have any questions.\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
